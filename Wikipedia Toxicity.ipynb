{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb11fdbe",
   "metadata": {},
   "source": [
    "# Wikipedia Toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76abe8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas package\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd238982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Load the data using read_csv function from pandas package\n",
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65adb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90244a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Get the comments into a list, for easy text cleanup and manipulation\n",
    "comments = data[\"comment_text\"]\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Cleanup\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb85900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords from nltk\n",
    "nltk_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40352385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords from sklearn\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1953ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the stopwords from sklearn & NLTK\n",
    "combined_stopwords = nltk_stopwords.union(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38dd1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the text\n",
    "def comment_clear(text):\n",
    "    # Using regular expressions, remove IP addresses\n",
    "    new_text = re.sub(r\"((\\d{1,3}\\.){3}\\d{1,3})$\", \"\", text)\n",
    "    # Using regular expressions, remove URLs\n",
    "    new_text = re.sub(r\"http\\S+\", \"\", new_text)\n",
    "    # Normalize the casing\n",
    "    new_text = new_text.lower().strip()\n",
    "    # Remove punctuation\n",
    "    new_text = re.sub(r\"[^a-zA-Z]\", \" \", new_text)\n",
    "    # Tokenize using word_tokenize from NLTK\n",
    "    new_text = [token for token in word_tokenize(new_text) if token not in combined_stopwords]\n",
    "    # removing words less than two charactors\n",
    "    new_text = [token for token in new_text if (len(token)>2)] \n",
    "    \n",
    "    cleaned_text = ''\n",
    "    for token in new_text:\n",
    "        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dee18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comments = list(comments.apply(comment_clear))\n",
    "clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fb1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['cleaned_comment_with_SW'] = clean_comments\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a65805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for t in data['cleaned_comment_with_SW']:\n",
    "    all_words.extend(t.split())\n",
    "\n",
    "# Frequency Distribution\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Top 25 most common words')\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "freq_dist.plot(25, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Using a counter, find the top terms in the data. \n",
    "#    Can any of these be considered contextual stop words? \n",
    "#    Words like “Wikipedia”, “page”, “edit” are examples of contextual stop words\n",
    "#    If yes, drop these from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50011014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider contextual stop words\n",
    "cleaned_comment_words = []\n",
    "for item in clean_comments:\n",
    "    cleaned_comment_words = cleaned_comment_words + item.split()\n",
    "\n",
    "comment_freq = nltk.FreqDist(cleaned_comment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87479ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the common 200 words by frequency\n",
    "comment_freq.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the contextual stop words in the data. \n",
    "domain_stopwords = ['article', 'page','wikipedia', 'edit', 'user','image' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the text with the contextual stop words\n",
    "def comment_clear_contextual_stopwords (text):\n",
    "    # Tokenize using word_tokenize from NLTK\n",
    "    new_text = [token for token in text.split() if token not in domain_stopwords]\n",
    "    \n",
    "    cleaned_text = ''\n",
    "    for token in new_text:\n",
    "        cleaned_text = cleaned_text + token + ' '\n",
    "    \n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the clearner function to clear text with contextual stop words\n",
    "clean_comments_domain = list(data['cleaned_comment_with_SW']\n",
    "                             .apply(comment_clear_contextual_stopwords))\n",
    "clean_comments_domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column with clean comments \n",
    "data['cleaned_comment'] = clean_comments_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the details of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for t in data['cleaned_comment']:\n",
    "    all_words.extend(t.split())\n",
    "\n",
    "# Frequency Distribution\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Top 25 most common words')\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "freq_dist.plot(25, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Separate into train and test sets\n",
    "\n",
    "# import the split package\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794be047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X & Y dataset\n",
    "X = data['cleaned_comment']\n",
    "y = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd354d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test sets with 70-30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88647651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Use TF-IDF values for the terms as feature to get into a vector space model\n",
    "\n",
    "#   Import TF-IDF vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bfc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Instantiate with a maximum of 4000 terms in your vocabulary\n",
    "TFIDF = TfidfVectorizer( min_df=5, max_features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Fit and apply on the train set\n",
    "X_train_vectorizer = TFIDF.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b03db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   display the feature names \n",
    "print(TFIDF.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(TFIDF.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c48830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Apply on the test set\n",
    "X_test_vectorizer = TFIDF.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply transform to the X\n",
    "X_vectorize = TFIDF.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a001420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Model building: Support Vector Machine\n",
    "\n",
    "#   Instantiate SVC from sklearn with a linear kernel\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Fit on the train data\n",
    "svc.fit(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Make predictions for the train and the test set\n",
    "#   Predict Y train\n",
    "Y_train_pred = svc.predict(X_train_vectorizer)\n",
    "Y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c027f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  predict Y Test\n",
    "Y_test_pred = svc.predict(X_test_vectorizer)\n",
    "Y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bcbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Model evaluation: Accuracy, recall, and f1_score\n",
    "\n",
    "# import pakages for metrics and reporting\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Report the accuracy on the train set\n",
    "accuracy_score(y_train, Y_train_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Get the f1_score on the train set\n",
    "print(classification_report(y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Report the recall on the train set:decent, high, low?\n",
    "# recall   - 0 class value is 1 (high), 1 class value is 0.71 (low)\n",
    "# f1-score - 0 class value is 0.99(high), 1 cluse value is 0.83 (decent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb3109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a9d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s\n",
    "\n",
    "#    Adjust the appropriate parameter in the SVC module\n",
    "svc1 = SVC(kernel='linear', class_weight='balanced', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Train again with the adjustment and evaluate\n",
    "\n",
    "#     Train the model on the train set\n",
    "svc1.fit(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluate the predictions on the validation set: accuracy, recall, f1_score\n",
    "#  predict Y Train\n",
    "Y_train_pred1 = svc1.predict(X_train_vectorizer)\n",
    "Y_train_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  predict Y Test\n",
    "Y_test_pred1 = svc1.predict(X_test_vectorizer)\n",
    "Y_test_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24194efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Report the accuracy of the train set\n",
    "accuracy_score(y_train, Y_train_pred1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cbcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Get the f1_score of the train set\n",
    "print(classification_report(y_train, Y_train_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Get the f1_score of the test set\n",
    "print(classification_report(y_test, Y_test_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Hyperparameter tuning\n",
    "\n",
    "#Import GridSearch and StratifiedKFold (because of class imbalance)\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d416c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SVC model\n",
    "svc_Hy = SVC(kernel='linear', class_weight='balanced', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11264a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply TF-IDF vectorizer to all comments\n",
    "X_vectorizer = TFIDF.fit_transform(data['cleaned_comment'])\n",
    "y = data['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the parameter grid to choose for ‘C’\n",
    "C_values = np.arange(0.00001, 1, 0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the SKFold \n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef69e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a balanced class weight while instantiating the Support Vector Classifier\n",
    "grid = GridSearchCV(estimator=svc_Hy, param_grid={'C': C_values}, cv=kfold, scoring='accuracy', \n",
    "                    return_train_score=True, verbose=2, n_jobs=-1)\n",
    "grid_results = grid.fit(X_vectorizer,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fe747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display best C parameter and scores\n",
    "grid_results.best_params_, grid_results.best_score_, grid_results.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac93af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de997c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_train_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb45737",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_test_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['std_test_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff368b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_results.cv_results_['mean_train_score'] - grid_results.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1752c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Find the parameters with the best recall in cross validation\n",
    "\n",
    "#  Choose ‘recall’ as the metric for scoring\n",
    "\n",
    "grid_recall = GridSearchCV(estimator=svc_Hy, param_grid={'C': C_values}, cv=kfold, scoring='recall', \n",
    "                    return_train_score=True, verbose=2, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose stratified 5 fold cross validation scheme \n",
    "grid_results_recall = grid_recall.fit(X_vectorizer,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31821b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_recall.best_params_, grid_results_recall.best_score_\n",
    ", grid_results_recall.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_recall.cv_results_['mean_train_score'][grid_results_recall.best_index_]*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ffcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_recall.cv_results_['mean_test_score'][grid_results_recall.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b18adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_recall.cv_results_['std_test_score'][grid_results_recall.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65445477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_results_recall.cv_results_['mean_train_score'] \n",
    "         - grid_results_recall.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. What are the best parameters?\n",
    "# The best C parameter is 0.05. Mean train score and mean test score difference is 1.7359."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d873538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Predict and evaluate using the best estimator\n",
    "\n",
    "#   What is the recall on the test set for the toxic comments?\n",
    "#   What is the f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ae7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Use best estimator from the grid search to make predictions on the test set\n",
    "model_final = SVC(kernel='linear', C=0.050010000000000006, class_weight='balanced', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_final.fit(X_train_vectorizer, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the values from the model\n",
    "y_train_predict_final = model_final.predict(X_train_vectorizer)\n",
    "y_train_predict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca234e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the values from the model\n",
    "y_test_predict_final = model_final.predict(X_test_vectorizer)\n",
    "y_test_predict_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56612594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Report the accuracy on the test set\n",
    "accuracy_score(y_train, y_train_predict_final)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_predict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_predict_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_test_predict_final, output_dict=True)['weighted avg']['recall']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_test_predict_final, output_dict=True)['weighted avg']['f1-score']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. What are the most prominent terms in the toxic comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Separate the comments from the test set that the model identified as toxic\n",
    "X_test[y_test_predict_final==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Make one large list of the terms\n",
    "merged_text = []\n",
    "for item in X_test[y_test_predict_final==1]:\n",
    "    merged_text = merged_text + item.split()\n",
    "    \n",
    "frequency_words = nltk.FreqDist(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    " frequency_words.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fe1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    Get the top 15 terms\n",
    "frequency_words.most_common(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
